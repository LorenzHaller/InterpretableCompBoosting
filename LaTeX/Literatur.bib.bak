% Encoding: UTF-8

@InProceedings{Lin14,
  author =    {Hsuan{-}Tien Lin},
  title =     {Reduction from Cost-Sensitive Multiclass Classification to One-versus-One Binary Classification},
  booktitle = {Proceedings of the Sixth Asian Conference on Machine Learning, {ACML} 2014, Nha Trang City, Vietnam, November 26-28, 2014.},
  year =      {2014},
  bibsource = {dblp computer science bibliography, http://dblp.org},
  biburl =    {http://dblp.uni-trier.de/rec/bib/conf/acml/Lin14},
  crossref =  {DBLP:conf/acml/2014},
  timestamp = {Tue, 29 Mar 2016 21:26:13 +0200},
  url =       {http://jmlr.org/proceedings/papers/v39/lin14.html}
}

@Article{bankmarketing,
  author =  {S. Moro and P. Cortez and P. Rita},
  title =   {A Data-Driven Approach to Predict the Success of Bank Telemarketing},
  journal = {Decision Support Systems},
  year =    {2014},
  pages =   {62:22-31}
}

@InProceedings{Beygelzimer:2005:ELR:1102351.1102358,
  author =    {Beygelzimer, Alina and Dani, Varsha and Hayes, Tom and Langford, John and Zadrozny, Bianca},
  title =     {Error Limiting Reductions Between Classification Tasks},
  booktitle = {Proceedings of the 22Nd International Conference on Machine Learning},
  year =      {2005},
  series =    {ICML '05},
  pages =     {49--56},
  address =   {New York, NY, USA},
  publisher = {ACM},
  acmid =     {1102358},
  doi =       {10.1145/1102351.1102358},
  isbn =      {1-59593-180-5},
  location =  {Bonn, Germany},
  numpages =  {8},
  url =       {http://doi.acm.org/10.1145/1102351.1102358}
}

@InBook{Beygelzimer2008,
  pages =     {3--28},
  title =     {Machine Learning Techniques---Reductions Between Prediction Quality Metrics},
  publisher = {Springer US},
  year =      {2008},
  author =    {Beygelzimer, Alina and Langford, John and Zadrozny, Bianca},
  editor =    {Liu, Zhen and Xia, Cathy H.},
  address =   {Boston, MA},
  booktitle = {Performance Modeling and Engineering},
  doi =       {10.1007/978-0-387-79361-0_1},
  isbn =      {978-0-387-79361-0},
  url =       {http://dx.doi.org/10.1007/978-0-387-79361-0_1}
}

@Article{Breiman:2001:RF:570181.570182,
  author =     {Breiman, Leo},
  title =      {Random Forests},
  journal =    {Mach. Learn.},
  year =       {2001},
  volume =     {45},
  number =     {1},
  pages =      {5--32},
  month =      oct,
  acmid =      {570182},
  address =    {Hingham, MA, USA},
  doi =        {10.1023/A:1010933404324},
  issn =       {0885-6125},
  issue_date = {October 1 2001},
  keywords =   {classification, ensemble, regression},
  numpages =   {28},
  publisher =  {Kluwer Academic Publishers},
  url =        {http://dx.doi.org/10.1023/A:1010933404324}
}

@Book{cart84-2,
  title =     {Classification and Regression Trees},
  publisher = {Wadsworth Publishing Company},
  year =      {1984},
  author =    {Leo {Breiman} and J. H. {Friedman} and R. A. {Olshen} and C. J. {Stone}},
  series =    {Statistics/Probability Series},
  address =   {Belmont, California, U.S.A.},
  isbn-hard = {0534980546 (softcover)},
  isbn-soft = {0534980538 (hardcover)}
}

@Misc{codeBahnsen,
  author =    {Alejandro Correa Bahnsen},
  title =     {Source code for costcla.datasets.base},
  timestamp = {13.07.2016},
  url =       {http://albahnsen.com/CostSensitiveClassification/_modules/costcla/datasets/base.html}
}

@Article{DBLP:journals/corr/BischlKKLMFHHLT15,
  author =    {Bernd Bischl and Pascal Kerschke and Lars Kotthoff and Marius Thomas Lindauer and Yuri Malitsky and Alexandre Fr{\'{e}}chette and Holger H. Hoos and Frank Hutter and Kevin Leyton{-}Brown and Kevin Tierney and Joaquin Vanschoren},
  title =     {ASlib: {A} Benchmark Library for Algorithm Selection},
  journal =   {CoRR},
  year =      {2015},
  volume =    {abs/1506.02465},
  bibsource = {dblp computer science bibliography, http://dblp.org},
  biburl =    {http://dblp.uni-trier.de/rec/bib/journals/corr/BischlKKLMFHHLT15},
  timestamp = {Wed, 01 Jul 2015 15:10:24 +0200},
  url =       {http://arxiv.org/abs/1506.02465}
}

@Unpublished{ediss12990,
  author =    {Manuel J. A. Eugster},
  title =     {Benchmark Experiments},
  month =     {M{\"a}rz},
  year =      {2011},
  publisher = {Ludwig-Maximilians-Universit{\"a}t M{\"u}nchen}
}

@Unpublished{ediss15058,
  author =    {Alexander Hapfelmeier},
  title =     {Analysis of missing data with random forests},
  note =      {-},
  month =     {Oktober},
  year =      {2012},
  abstract =  {Random Forests are widely used for data prediction and interpretation purposes.  They
show many appealing characteristics, such as the ability to deal with high dimensional data,
complex interactions and correlations. Furthermore, missing values can easily be processed
by the built-in procedure of surrogate splits. However, there is only little knowledge about
the properties of recursive partitioning in missing data situations.  Therefore, extensive
simulation studies and empirical evaluations have been conducted to gain deeper insight.
In addition, new methods have been developed to enhance methodology and solve current
issues of data interpretation, prediction and variable selection.
A variable?s relevance in a Random Forest can be assessed by means of importance
measures. Unfortunately, existing methods cannot be applied when the data contain miss-
ing values.  Thus, one of the most appreciated properties of Random Forests ? its ability
to handle missing values ? gets lost for the computation of such measures.   This work
presents a new approach that is designed to deal with missing values in an intuitive and
straightforward way, yet retains widely appreciated qualities of existing methods. Results
indicate that it meets sensible requirements and shows good variable ranking properties.
Random Forests provide variable selection that is usually based on importance mea-
sures.  An extensive review of corresponding literature led to the development of a new
approach that is based on a profound theoretical framework and meets important statis-
tical properties.  A comparison to another eight popular methods showed that it controls
the test-wise and family-wise error rate, provides a higher power to distinguish relevant
from non-relevant variables and leads to models located among the best performing ones.
Alternative ways to handle missing values are the application of imputation methods
and complete case analysis.  Yet it is unknown to what extent these approaches are able
to provide sensible variable rankings and meaningful variable selections.   Investigations
showed that complete case analysis leads to inaccurate variable selection as it may in-
appropriately penalize the importance of fully observed variables.  By contrast, the new
importance measure decreases for variables with missing values and therefore causes se-
lections that accurately re?ect the information given in actual data situations.  Multiple
imputation leads to an assessment of a variable?s importance and to selection frequencies
that would be expected for data that was completely observed.  In several performance
evaluations the best prediction accuracy emerged from multiple imputation, closely fol-
lowed by the application of surrogate splits.   Complete case analysis clearly performed
worst.},
  publisher = {Ludwig-Maximilians-Universit{\"a}t M{\"u}nchen},
  url =       {http://nbn-resolving.de/urn:nbn:de:bvb:19-150588}
}

@InProceedings{Elkan01thefoundations,
  author =    {Charles Elkan},
  title =     {The Foundations of Cost-Sensitive Learning},
  booktitle = {In Proceedings of the Seventeenth International Joint Conference on Artificial Intelligence},
  year =      {2001},
  pages =     {973--978}
}

@Article{Hapfelmeier:2013:NVS:2422618.2422689,
  author =     {Hapfelmeier, A. and Ulm, K.},
  title =      {A New Variable Selection Approach Using Random Forests},
  journal =    {Comput. Stat. Data Anal.},
  year =       {2013},
  volume =     {60},
  pages =      {50--69},
  month =      apr,
  acmid =      {2422689},
  address =    {Amsterdam, The Netherlands, The Netherlands},
  doi =        {10.1016/j.csda.2012.09.020},
  issn =       {0167-9473},
  issue_date = {April, 2013},
  keywords =   {Multiple testing, Permutation tests, Random Forests, Variable selection},
  numpages =   {20},
  publisher =  {Elsevier Science Publishers B. V.},
  url =        {http://dx.doi.org/10.1016/j.csda.2012.09.020}
}

@Book{hastie01statisticallearning,
  title =     {The Elements of Statistical Learning},
  publisher = {Springer New York Inc.},
  year =      {2001},
  author =    {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
  series =    {Springer Series in Statistics},
  address =   {New York, NY, USA},
  added-at =  {2008-05-16T16:17:42.000+0200},
  biburl =    {http://www.bibsonomy.org/bibtex/2f58afc5c9793fcc8ad8389824e57984c/sb3000},
  interhash = {d585aea274f2b9b228fc1629bc273644},
  intrahash = {f58afc5c9793fcc8ad8389824e57984c},
  keywords =  {ml statistics},
  timestamp = {2008-05-16T16:17:43.000+0200}
}

@Article{journals/eswa/BahnsenAO15,
  author =    {Bahnsen, Alejandro Correa and Aouada, Djamila and Ottersten, Bj√∂rn E.},
  title =     {Example-dependent cost-sensitive decision trees.},
  journal =   {Expert Syst. Appl.},
  year =      {2015},
  volume =    {42},
  number =    {19},
  pages =     {6609-6619},
  added-at =  {2015-06-27T00:00:00.000+0200},
  biburl =    {http://www.bibsonomy.org/bibtex/2084eff6fcfd9c16761f23677a076a160/dblp},
  ee =        {http://dx.doi.org/10.1016/j.eswa.2015.04.042},
  interhash = {33fe25a8200e15ab77c7444a4377b89c},
  intrahash = {084eff6fcfd9c16761f23677a076a160},
  keywords =  {dblp},
  timestamp = {2015-06-30T11:33:23.000+0200},
  url =       {http://dblp.uni-trier.de/db/journals/eswa/eswa42.html#BahnsenAO15}
}

@Article{journals/sac/HapfelmeierHUS14,
  author =    {Hapfelmeier, Alexander and Hothorn, Torsten and Ulm, Kurt and Strobl, Carolin},
  title =     {A new variable importance measure for random forests with missing data.},
  journal =   {Statistics and Computing},
  year =      {2014},
  volume =    {24},
  number =    {1},
  pages =     {21-34},
  added-at =  {2014-01-15T00:00:00.000+0100},
  biburl =    {http://www.bibsonomy.org/bibtex/2f7f804c2c6178b344f530d24a3beccce/dblp},
  ee =        {http://dx.doi.org/10.1007/s11222-012-9349-1},
  interhash = {7102d05cce491414edcaa291ea1853c8},
  intrahash = {f7f804c2c6178b344f530d24a3beccce},
  keywords =  {dblp},
  timestamp = {2014-01-16T11:33:56.000+0100},
  url =       {http://dblp.uni-trier.de/db/journals/sac/sac24.html#HapfelmeierHUS14}
}

@Misc{mlrCosts,
  author =    {Bernd Bischl},
  title =     {Cost Sensitive Classification: Example-dependent misclassification costs},
  timestamp = {12.07.2016},
  url =       {https://mlr-org.github.io/mlr-tutorial/release/html/cost_sensitive_classif/index.html}
}

@Misc{mlrMeasures,
  author =    {Bernd Bischl},
  title =     {Implemented Performance Measures},
  timestamp = {12.07.2016},
  url =       {https://mlr-org.github.io/mlr-tutorial/release/html/measures/index.html}
}

@Book{Quinlan:1993:CPM:152181,
  title =     {C4.5: Programs for Machine Learning},
  publisher = {Morgan Kaufmann Publishers Inc.},
  year =      {1993},
  author =    {Quinlan, J. Ross},
  address =   {San Francisco, CA, USA},
  isbn =      {1-55860-238-0}
}

@Manual{rpartuser15,
  title =  {User written splitting functions for RPART},
  author = {Terry Thereneau and Mayo Clinic},
  year =   {2015}
}

@Misc{Strobl,
  author =   {Carolin Strobl and James Malley and Gerhard Tutz},
  title =    {An Introduction to Recursive Partitioning: Rationale, Application and Characteristics of Classification and Regression Trees, Bagging and Random Forests},
  year =     {2009},
  abstract = {Recursive partitioning methods have become popular and widely used tools for nonparametric regression and classification in many scientific fields. Especially random forests, that can deal with large numbers of predictor variables even in the presence of complex interactions, have been applied successfully in genetics, clinical medicine and bioinformatics within the past few years.

High dimensional problems are common not only in genetics, but also in some areas of psychological research, where only few subjects can be measured due to time or cost constraints, yet a large amount of data is generated for each subject. Random forests have been shown to achieve a high prediction accuracy in such applications, and provide descriptive variable importance measures reflecting the impact of each variable in both main effects and interactions.

The aim of this work is to introduce the principles of the standard recursive partitioning methods as well as recent methodological improvements, to illustrate their usage for low and high dimensional data exploration, but also to point out limitations of the methods and potential pitfalls in their practical application.

Application of the methods is illustrated using freely available implementations in the R system for statistical computing.},
  keyword =  {CART, C4.5, bootstrap, variable selection, variable importance},
  series =   {tech},
  url =      {http://nbn-resolving.de/urn/resolver.pl?urn=nbn:de:bvb:19-epub-10589-8},
  volume =   {55}
}

@InProceedings{Strobl2008a,
  author =    {Strobl, C. and Zeileis, A.},
  title =     {Danger: High power! Exploring the statistical properties of a test for random forest variable importance},
  booktitle = {Proceedings of the 18th International Conference on Computational Statistics},
  year =      {2008},
  address =   {Porto, Portugal},
  added-at =  {2010-07-07T17:27:19.000+0200},
  biburl =    {http://www.bibsonomy.org/bibtex/26d5ac8af4c795c49d5e7942773e7ddc0/pillo},
  interhash = {ce44088f0b647f60975077aa18d967cd},
  intrahash = {6d5ac8af4c795c49d5e7942773e7ddc0},
  keywords =  {imported},
  owner =     {Bernd Panassiti},
  timestamp = {2010-07-07T17:27:24.000+0200}
}

@Book{woll2010,
  title =     {Grundlagen der Datenanalyse mit R: eine anwendungsorientierte Einf√ºhrung.},
  publisher = {Springer},
  year =      {2010},
  author =    {Daniel Wollschl√§ger}
}

@InProceedings{Zadrozny:2003:CLC:951949.952181,
  author =    {Zadrozny, Bianca and Langford, John and Abe, Naoki},
  title =     {Cost-Sensitive Learning by Cost-Proportionate Example Weighting},
  booktitle = {Proceedings of the Third IEEE International Conference on Data Mining},
  year =      {2003},
  series =    {ICDM '03},
  pages =     {435--},
  address =   {Washington, DC, USA},
  publisher = {IEEE Computer Society},
  acmid =     {952181},
  isbn =      {0-7695-1978-4},
  url =       {http://dl.acm.org/citation.cfm?id=951949.952181}
}
